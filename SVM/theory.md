# ðŸ“˜ Soft-Margin SVM â€” Theory & Derivation

We want to find a hyperplane:

$$f(x) = w^T x + b$$

that separates data with the **largest margin**.

---

## 1. Hard Margin (perfectly separable case)

Optimization problem:

$$min_{w,b} (1/2) * ||w||^2$$


subject to:

$$y_i (w^T x_i + b) >= 1, for all i
$$

- Ensures margin = `1 / ||w||`.
- Fails when classes overlap.

---

## 2. Soft Margin (realistic case)

Introduce **slack variables** `Î¾_i >= 0` to allow violations:
$$
y_i (w^T x_i + b) >= 1 - Î¾_i
$$
- $Î¾áµ¢ = 0 â†’$ correctly classified and outside margin  
- $0 < Î¾áµ¢ < 1 â†’$ inside margin, but still correct side  
- $Î¾áµ¢ > 1 â†’$ misclassified point  

---

## 3. New Optimization Problem

We now minimize both:
- Large weights (small ||w||Â²) â†’ wide margin  
- Total slack (Î£ Î¾áµ¢) â†’ fewer violations  
$$
min_{w, b, Î¾} (1/2) * ||w||^2 + C * Î£ Î¾_i
$$
subject to:
$$
y_i (w^T x_i + b) >= 1 - Î¾_i
Î¾_i >= 0
$$

- `C > 0`: regularization parameter (penalty for misclassification).  
  - Large C â†’ less tolerance, stricter  
  - Small C â†’ more tolerance, wider margin  

---

## 4. Hinge Loss Form

Instead of explicitly using Î¾áµ¢, we reformulate with **hinge loss**:
$$
Loss(w, b) = (1/2) * ||w||^2 + C * Î£ max(0, 1 - y_i (w^T x_i + b))
$$
- If correctly classified with margin â‰¥ 1 â†’ loss = 0  
- Otherwise â†’ loss increases linearly with violation  

---

## 5. Gradient (for Implementation)

We minimize:
$$
J(w, b) = (1/2) * ||w||^2 + C * Î£ max(0, 1 - y_i (w^T x_i + b))
$$


Case 1: if `y_i (w^T x_i + b) >= 1`
$$âˆ‡w = w$$
$$
âˆ‡b = 0
$$
Case 2: if `y_i (w^T x_i + b) < 1`
$$âˆ‡w = w - C * y_i * x_i$$
$$âˆ‡b = -C * y_i
$$
So we can use **gradient descent / SGD** to learn `w, b`.

---
